{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a81250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from toolz import curry\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "# until I properly package this library, need to allow for relative imports\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from bertron.train import input_dict, evaluate\n",
    "\n",
    "seed = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf22c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249f4f2",
   "metadata": {},
   "source": [
    "## load model and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4eafa52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=33, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/processed/labels.json', 'r') as f_in:\n",
    "    label_dict = json.load(f_in)\n",
    "    \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label_dict),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# load transfer learning component for Enron corpus\n",
    "checkpoint = torch.load('../bertron/models/finetuned_BERT_epoch_1.model')\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5eebd8",
   "metadata": {},
   "source": [
    "### until I make a proper pipeline, regenerate tokenizer and dataloaders manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6174773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "max_len = 512\n",
    "tokenizer_cur = curry(tokenizer.batch_encode_plus)(\n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='max_length', \n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a16cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet('../data/processed/X.parquet').sample(frac=.05)\n",
    "\n",
    "seed = 666\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.index.values, \n",
    "    X['label'].values, \n",
    "    test_size=0.15, \n",
    "    random_state=seed, \n",
    "    stratify=X['label'].values\n",
    ")\n",
    "\n",
    "X['data_type'] = np.nan * X.shape[0]\n",
    "X.loc[X_train, 'data_type'] = 'train'\n",
    "X.loc[X_test, 'data_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e3d394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_test = X['data_type']=='test'\n",
    "encoded_data_test = tokenizer_cur(\n",
    "    X[mask_test]['body'].values\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(X[mask_test]['label'].values)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test, \n",
    "    sampler=SequentialSampler(dataset_test), \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1df0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "indices = torch.arange(10)\n",
    "tr_10 = data_utils.Subset(dataloader_test, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147eb38",
   "metadata": {},
   "source": [
    "## sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "227edaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "making output\n",
      "finished one batch\n"
     ]
    }
   ],
   "source": [
    "loss_test_total = 0\n",
    "predictions, true_tests = [], []\n",
    "\n",
    "n = 0\n",
    "for batch in dataloader_test:\n",
    "    # just a single batch\n",
    "    if n < 1:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = input_dict(batch)\n",
    "\n",
    "        print('making output')\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_test_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_tests.append(label_ids)\n",
    "        print('finished one batch')\n",
    "        n += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7d70c9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>recip_primary</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31085</th>\n",
       "      <td>chris.germany</td>\n",
       "      <td>marde.driscoll</td>\n",
       "      <td>Re: Question</td>\n",
       "      <td>I luv trucks.  I was injured on Sunday,  I use crutches now.\\n\\n\\nTo: Chris Germany/HOU/ECT@ECT\\ncc:  \\nSubject: Question\\n\\nHey, I'm going to have to start car shopping.  I'm really thinking about a \\ntruck.  Are you interested in helping me look?  I'm asking because I know how \\nmuch you like to look at trucks!  md</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62600</th>\n",
       "      <td>john.lavorato</td>\n",
       "      <td>stanley.horton</td>\n",
       "      <td>RE:</td>\n",
       "      <td>Stan\\n\\nI met with El Paso yesterday.  They seem very interested.\\n\\nThanks</td>\n",
       "      <td>30</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77954</th>\n",
       "      <td>kay.mann</td>\n",
       "      <td>jeffrey.hodge</td>\n",
       "      <td>Gas supply for VEPCO project</td>\n",
       "      <td>Hi Jeff,\\n\\nOzzie Pagan's team is working on a project for VEPCO.  The power plant will \\nbe located in Edgecombe County, North Carolina. I've been told that getting \\ngas for the plant is a significant issue due to local gas supply conditions.  \\nOne alternative has been to buy gas from the City of Rocky Mount, which has a \\ntake or pay contract with the local supplier, and is not using all of their \\ngas, especially in the summer.  \\n\\nWe have developed a preliminary draft of the LOI, which I am attaching for \\nyour review.  I would appreciate the opportunity to discuss it with you to \\nget your input, etc.  Of particular concern is that Rocky Mount wants ENA to \\ndeal exclusively with them during the term of the LOI (this isn't reflected \\nin the draft).\\n\\nThanks,\\n\\nKay</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88012</th>\n",
       "      <td>debra.perlingiere</td>\n",
       "      <td>tammi.depaolis</td>\n",
       "      <td>Re: Lunch</td>\n",
       "      <td>Thanks Monday is good for me.\\n\\n\\nDebra Perlingiere\\nEnron North America Corp.\\nLegal Department\\n1400 Smith Street, EB 3885\\nHouston, Texas 77002\\ndperlin@enron.com\\nPhone 713-853-7658\\nFax  713-646-3490</td>\n",
       "      <td>10</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author   recip_primary                       subject  \\\n",
       "31085      chris.germany  marde.driscoll                  Re: Question   \n",
       "62600      john.lavorato  stanley.horton                           RE:   \n",
       "77954           kay.mann   jeffrey.hodge  Gas supply for VEPCO project   \n",
       "88012  debra.perlingiere  tammi.depaolis                     Re: Lunch   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    body  \\\n",
       "31085                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     I luv trucks.  I was injured on Sunday,  I use crutches now.\\n\\n\\nTo: Chris Germany/HOU/ECT@ECT\\ncc:  \\nSubject: Question\\n\\nHey, I'm going to have to start car shopping.  I'm really thinking about a \\ntruck.  Are you interested in helping me look?  I'm asking because I know how \\nmuch you like to look at trucks!  md   \n",
       "62600                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Stan\\n\\nI met with El Paso yesterday.  They seem very interested.\\n\\nThanks   \n",
       "77954  Hi Jeff,\\n\\nOzzie Pagan's team is working on a project for VEPCO.  The power plant will \\nbe located in Edgecombe County, North Carolina. I've been told that getting \\ngas for the plant is a significant issue due to local gas supply conditions.  \\nOne alternative has been to buy gas from the City of Rocky Mount, which has a \\ntake or pay contract with the local supplier, and is not using all of their \\ngas, especially in the summer.  \\n\\nWe have developed a preliminary draft of the LOI, which I am attaching for \\nyour review.  I would appreciate the opportunity to discuss it with you to \\nget your input, etc.  Of particular concern is that Rocky Mount wants ENA to \\ndeal exclusively with them during the term of the LOI (this isn't reflected \\nin the draft).\\n\\nThanks,\\n\\nKay   \n",
       "88012                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Thanks Monday is good for me.\\n\\n\\nDebra Perlingiere\\nEnron North America Corp.\\nLegal Department\\n1400 Smith Street, EB 3885\\nHouston, Texas 77002\\ndperlin@enron.com\\nPhone 713-853-7658\\nFax  713-646-3490   \n",
       "\n",
       "       label data_type  \n",
       "31085      2      test  \n",
       "62600     30      test  \n",
       "77954      0      test  \n",
       "88012     10      test  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "X[mask_test][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5410f207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true authors     : ['chris.germany', 'john.lavorato', 'kay.mann', 'debra.perlingiere']\n",
      "predicted authors: ['chris.germany', 'chris.germany', 'kay.mann', 'debra.perlingiere']\n"
     ]
    }
   ],
   "source": [
    "true_pred = label_ids\n",
    "author_pred = [p.argmax() for p in predictions[0]]\n",
    "label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "print(f'true authors     : {[label_dict_inverse[x] for x in true_pred]}')\n",
    "print(f'predicted authors: {[label_dict_inverse[x] for x in author_pred]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e97624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so much more work to do to make this better..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
